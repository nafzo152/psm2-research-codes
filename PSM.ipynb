{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
      "0            -342            -200              41             328   \n",
      "1             -87            -248             262             295   \n",
      "2             -62             -23              -7             142   \n",
      "3              22            -153              17             276   \n",
      "4              86             -36            -141             252   \n",
      "\n",
      "   AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
      "0            -224             -427             -656            -292   \n",
      "1            -226             -493              367            -452   \n",
      "2            -233             -284             -167             -97   \n",
      "3            -211             -250               55            -141   \n",
      "4            -201             -384             -420            -197   \n",
      "\n",
      "   AFFX-CreX-3_at  AFFX-BioB-5_st  ...  U58516_at  U73738_at  X06956_at  \\\n",
      "0             137            -144  ...       1023         67        214   \n",
      "1             194             162  ...        529       -295        352   \n",
      "2             -12             -70  ...        383         46        104   \n",
      "3               0             500  ...        399         16        558   \n",
      "4             -60            -468  ...         91        -84        615   \n",
      "\n",
      "   X16699_at  X83863_at  Z17240_at  L49218_f_at  M71243_f_at  Z78285_f_at  \\\n",
      "0       -135       1074        475           48          168          -70   \n",
      "1        -67         67        263          -33          -33          -21   \n",
      "2         15        245        164           84          100          -18   \n",
      "3         24        893        297            6         1971          -42   \n",
      "4        -52       1235          9            7         1545          -81   \n",
      "\n",
      "   CLASS  \n",
      "0    ALL  \n",
      "1    ALL  \n",
      "2    ALL  \n",
      "3    ALL  \n",
      "4    ALL  \n",
      "\n",
      "[5 rows x 7130 columns]\n",
      "AFFX-BioB-5_at    0\n",
      "AFFX-BioB-M_at    0\n",
      "AFFX-BioB-3_at    0\n",
      "AFFX-BioC-5_at    0\n",
      "AFFX-BioC-3_at    0\n",
      "                 ..\n",
      "Z17240_at         0\n",
      "L49218_f_at       0\n",
      "M71243_f_at       0\n",
      "Z78285_f_at       0\n",
      "CLASS             0\n",
      "Length: 7130, dtype: int64\n",
      "AFFX-BioB-5_at    0.0\n",
      "AFFX-BioB-M_at    0.0\n",
      "AFFX-BioB-3_at    0.0\n",
      "AFFX-BioC-5_at    0.0\n",
      "AFFX-BioC-3_at    0.0\n",
      "                 ... \n",
      "Z17240_at         0.0\n",
      "L49218_f_at       0.0\n",
      "M71243_f_at       0.0\n",
      "Z78285_f_at       0.0\n",
      "CLASS             0.0\n",
      "Length: 7130, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('binary.arff.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values in the entire DataFrame\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values)\n",
    "\n",
    "# Check for missing values percentage for each column\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "print(missing_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataset is in a CSV file named 'leukemia_dataset.csv'\n",
    "# You might need to change the file name or path according to your dataset location\n",
    "dataset = pd.read_csv('binary.arff.csv')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = dataset.isnull().sum()\n",
    "\n",
    "# Filter out columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "if columns_with_missing_values.empty:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(columns_with_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is not balanced.\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataset is in a CSV file named 'leukemia_dataset.csv'\n",
    "# You might need to change the file name or path according to your dataset location\n",
    "class_distribution = dataset['CLASS'].value_counts()\n",
    "\n",
    "# Check if the dataset is balanced\n",
    "is_balanced = (class_distribution.min() / class_distribution.max()) >= 0.8\n",
    "\n",
    "if is_balanced:\n",
    "    print(\"The dataset is balanced.\")\n",
    "else:\n",
    "    print(\"The dataset is not balanced.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Replace 'leukemia_data.csv' with the actual path to your dataset\n",
    "data = pd.read_csv('leukemia_data.csv')\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Assuming the last column is the label and all other columns are features\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Labels\n",
    "\n",
    "# Standardize the features (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets with an 80:20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train a Support Vector Classifier with hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Instantiate the SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict the labels on the testing set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Step 6: Calculate the accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
