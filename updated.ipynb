{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('binary.arff.csv')\n",
    "\n",
    "# Split data into X (features) and Y (target)\n",
    "X = df.iloc[:, :-1].values  # Features\n",
    "Y = df.iloc[:, -1].values   # Labels\n",
    "\n",
    "# Convert categorical labels to binary labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # Encode 'ALL' as 0 and 'AML' as 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "display(df.head())\n",
    "\n",
    "# columns\n",
    "display(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=14)\n",
    "\n",
    "# Build the model\n",
    "svm = SVC(kernel='linear',  random_state=12)\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "classification_rep = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Score for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform n-fold cross-validation\n",
    "n = 2\n",
    "cv_scores = cross_val_score(svm, X, Y, cv=n)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validation scores\n",
    "print(\"\\nCross-validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Plot Cross-validation Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, n + 1), cv_scores, color='skyblue')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-validation Scores')\n",
    "plt.ylim(0, 1)  # Limit y-axis to 0-1 for accuracy scores\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = metrics.roc_auc_score(Y_test, Y_pred)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificaion Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification report\n",
    "classification_rep = classification_report(Y_test, Y_pred, target_names=le.classes_, output_dict=True)\n",
    "\n",
    "# Extract precision, recall, and f1-score values\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "classes = list(le.classes_)\n",
    "values = {metric: [classification_rep[class_name][metric] for class_name in classes] for metric in metrics}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar positions and width\n",
    "bar_width = 0.2\n",
    "index = range(len(classes))\n",
    "\n",
    "# Plot bars for each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar([pos + i * bar_width for pos in index], values[metric], bar_width, label=metric)\n",
    "\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Report Metrics')\n",
    "plt.xticks([pos + bar_width for pos in index], classes)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Build the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=3, random_state=13)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "classification_rep = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Score for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform n-fold cross-validation\n",
    "n = 5\n",
    "cv_scores = cross_val_score(rf, X, Y, cv=n)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validation scores\n",
    "print(\"\\nCross-validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Plot Cross-validation Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, n + 1), cv_scores, color='skyblue')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-validation Scores')\n",
    "plt.ylim(0, 1)  # Limit y-axis to 0-1 for accuracy scores\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = metrics.roc_auc_score(Y_test, Y_pred)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification report\n",
    "classification_rep = classification_report(Y_test, Y_pred, target_names=le.classes_, output_dict=True)\n",
    "\n",
    "# Extract precision, recall, and f1-score values\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "classes = list(le.classes_)\n",
    "values = {metric: [classification_rep[class_name][metric] for class_name in classes] for metric in metrics}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar positions and width\n",
    "bar_width = 0.2\n",
    "index = range(len(classes))\n",
    "\n",
    "# Plot bars for each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar([pos + i * bar_width for pos in index], values[metric], bar_width, label=metric)\n",
    "\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Report Metrics')\n",
    "plt.xticks([pos + bar_width for pos in index], classes)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "# Compute mutual information between each feature and the target\n",
    "mi = mutual_info_classif(X, Y)\n",
    "\n",
    "# Sort features based on mutual information\n",
    "mi_sorted_indices = np.argsort(mi)[::-1]  # Sort in descending order\n",
    "\n",
    "# Get top 10 features based on mutual information\n",
    "top_10_mi_features = df.columns[mi_sorted_indices[:10]]\n",
    "\n",
    "print(\"Top 10 features based on Mutual Information:\")\n",
    "print(top_10_mi_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anova F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Perform ANOVA F-test\n",
    "f_scores, p_values = f_classif(X, Y)\n",
    "\n",
    "# Sort features based on F-scores\n",
    "f_sorted_indices = np.argsort(f_scores)[::-1]\n",
    "\n",
    "# Get top 10 features based on ANOVA F-test (for example)\n",
    "top_10_f_features = df.columns[f_sorted_indices[:10]]\n",
    "\n",
    "print(\"Top 10 features based on ANOVA F-test:\")\n",
    "print(top_10_f_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Chi-square test requires non-negative values, so we scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi_scores, p_values = chi2(X_scaled, Y)\n",
    "\n",
    "# Sort features based on Chi-square scores\n",
    "chi_sorted_indices = np.argsort(chi_scores)[::-1]\n",
    "\n",
    "# Get top 10 features based on Chi-square test (for example)\n",
    "top_10_chi_features = df.columns[chi_sorted_indices[:10]]\n",
    "\n",
    "print(\"Top 10 features based on Chi-Square Test:\")\n",
    "print(top_10_chi_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Top Feature Names to Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to indices\n",
    "def get_feature_indices(df, top_features):\n",
    "    return [df.columns.get_loc(feature) for feature in top_features]\n",
    "\n",
    "# Get top 10 features for each selection technique\n",
    "top_10_mi_indices = get_feature_indices(df, top_10_mi_features)\n",
    "top_10_f_indices = get_feature_indices(df, top_10_f_features)\n",
    "top_10_chi_indices = get_feature_indices(df, top_10_chi_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Model Performance After Filter Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to evaluate the SVM model\n",
    "def evaluate_svm(X, Y, top_features, technique_name):\n",
    "    # Filter the dataset with the top features\n",
    "    X_selected = X[:, top_features]\n",
    "    \n",
    "    # Split the dataset into training and test data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=14)\n",
    "    \n",
    "    # Build the SVM model\n",
    "    svm = SVC(kernel='linear', random_state=12)\n",
    "    \n",
    "    # Train the model\n",
    "    svm.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    Y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Print classification report\n",
    "    classification_rep = classification_report(Y_test, Y_pred)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    # Display the metrics with the technique name\n",
    "    print(f\"SVM Performance using {technique_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate SVM using the top 10 features selected by Mutual Information\n",
    "evaluate_svm(X, Y, top_10_mi_indices, \"Mutual Information\")\n",
    "\n",
    "# Evaluate SVM using the top 10 features selected by F-Score\n",
    "evaluate_svm(X, Y, top_10_f_indices, \"F-Score\")\n",
    "\n",
    "# Evaluate SVM using the top 10 features selected by Chi-Square\n",
    "evaluate_svm(X, Y, top_10_chi_indices, \"Chi-Square\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model Performance After Filter Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to evaluate the Random Forest model\n",
    "def evaluate_rf(X, Y, top_features, technique_name):\n",
    "    # Filter the dataset with the top features\n",
    "    X_selected = X[:, top_features]\n",
    "    \n",
    "    # Split the dataset into training and test data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=13)\n",
    "    \n",
    "    # Build the Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "    \n",
    "    # Train the model\n",
    "    rf.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    Y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Print classification report\n",
    "    classification_rep = classification_report(Y_test, Y_pred)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    # Display the metrics with the technique name\n",
    "    print(f\"Random Forest Performance using {technique_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate Random Forest using the top 10 features selected by Mutual Information\n",
    "evaluate_rf(X, Y, top_10_mi_indices, \"Mutual Information\")\n",
    "\n",
    "# Evaluate Random Forest using the top 10 features selected by F-Score\n",
    "evaluate_rf(X, Y, top_10_f_indices, \"F-Score\")\n",
    "\n",
    "# Evaluate Random Forest using the top 10 features selected by Chi-Square\n",
    "evaluate_rf(X, Y, top_10_chi_indices, \"Chi-Square\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 100 features:\n",
      "Index(['M63379_at', 'M63138_at', 'U22376_cds2_s_at', 'X61587_at', 'M14636_at',\n",
      "       'M84526_at', 'U57721_at', 'M23197_at', 'L09209_s_at', 'D88422_at',\n",
      "       'M16652_s_at', 'Z49194_at', 'M31523_at', 'M27891_at', 'M74088_s_at',\n",
      "       'D38073_at', 'U90549_at', 'M11722_at', 'U05259_rna1_at', 'X62320_at',\n",
      "       'M16038_at', 'HG4582-HT4987_at', 'X78669_at', 'X13973_at', 'L11669_at',\n",
      "       'X59417_at', 'U41635_at', 'M63838_s_at', 'U62136_at', 'L21954_at',\n",
      "       'U36621_cds2_at', 'M29474_at', 'M29696_at', 'HG2591-HT2687_s_at',\n",
      "       'J05243_at', 'X66533_at', 'M19507_at', 'U10868_at', 'M32304_s_at',\n",
      "       'M83652_s_at', 'X12451_at', 'HG2981-HT3127_s_at', 'Z15115_at',\n",
      "       'X85116_rna1_s_at', 'U51127_at', 'Y00787_s_at', 'U56833_at',\n",
      "       'X82240_rna1_at', 'L19437_at', 'S50223_at', 'U09578_at', 'M27783_s_at',\n",
      "       'U34877_at', 'D28532_at', 'X52142_at', 'M26708_s_at', 'M55150_at',\n",
      "       'U60319_at', 'X12447_at', 'D87076_at', 'M63959_at', 'U94836_at',\n",
      "       'S76617_at', 'L19872_at', 'AB002559_at', 'U00802_s_at', 'U78628_at',\n",
      "       'U46751_at', 'X51521_at', 'M26856_s_at', 'U25041_at', 'D63880_at',\n",
      "       'M13452_s_at', 'X52056_at', 'M62505_at', 'S73885_s_at', 'X53296_s_at',\n",
      "       'U29680_at', 'D63391_at', 'D80001_at', 'AB000449_at', 'M14199_s_at',\n",
      "       'U46499_at', 'X95735_at', 'L38593_s_at', 'AC002464_at', 'D80000_at',\n",
      "       'X90858_at', 'D49738_at', 'U57094_at', 'L76702_at', 'U51990_at',\n",
      "       'D87448_at', 'D86956_at', 'X79568_at', 'K01396_at', 'X74570_at',\n",
      "       'M92287_at', 'M93650_at', 'U88666_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('binary.arff.csv')\n",
    "\n",
    "# Split data into X (features) and Y (target)\n",
    "X = df.iloc[:, :-1].values  # Features\n",
    "Y = df.iloc[:, -1].values   # Labels\n",
    "\n",
    "# Convert categorical labels to binary labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # Encode 'ALL' as 0 and 'AML' as 1\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and fit the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Get the feature importance\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = importances.argsort()[::-1]\n",
    "\n",
    "# Select the top n most important features (for example, the top 500 features)\n",
    "n_features_to_select = 100  # Adjust this number based on your requirement\n",
    "top_n_indices = sorted_indices[:n_features_to_select]\n",
    "\n",
    "# Select the top features from the original dataset\n",
    "X_selected = X[:, top_n_indices]\n",
    "\n",
    "# Print the names of the selected features\n",
    "selected_feature_names = df.columns[top_n_indices]\n",
    "print(\"Selected top {} features:\".format(n_features_to_select))\n",
    "print(selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE Using Random Forest After Feature Importance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after RFE: 0.9333333333333333\n",
      "Feature ranking after RFE:\n",
      "[ 1  1  1  1  1  1  1  1  1  1 20  1  1  1  1 48  1  1  1  1  1 10 49  1\n",
      " 13  1  1  1 18 37  1  1  1 27  1 33  1 36  1  1  1  1 41  1 29  1 39 35\n",
      " 45 50 34  1 15 51 16  1  1  7  9 11  4 28  1 21 24 19 44 12 14 30 32  1\n",
      "  1 42 22 43  1  1  8 26 31 40  1  1 38 25 23  1  5  1  6  1  3  1  1  2\n",
      " 17  1 46 47]\n",
      "Selected features after RFE:\n",
      "Index(['M63379_at', 'M63138_at', 'U22376_cds2_s_at', 'X61587_at', 'M14636_at',\n",
      "       'M84526_at', 'U57721_at', 'M23197_at', 'L09209_s_at', 'D88422_at',\n",
      "       'Z49194_at', 'M31523_at', 'M27891_at', 'M74088_s_at', 'U90549_at',\n",
      "       'M11722_at', 'U05259_rna1_at', 'X62320_at', 'M16038_at', 'X13973_at',\n",
      "       'X59417_at', 'U41635_at', 'M63838_s_at', 'U36621_cds2_at', 'M29474_at',\n",
      "       'M29696_at', 'J05243_at', 'M19507_at', 'M32304_s_at', 'M83652_s_at',\n",
      "       'X12451_at', 'HG2981-HT3127_s_at', 'X85116_rna1_s_at', 'Y00787_s_at',\n",
      "       'M27783_s_at', 'M26708_s_at', 'M55150_at', 'S76617_at', 'D63880_at',\n",
      "       'M13452_s_at', 'X53296_s_at', 'U29680_at', 'U46499_at', 'X95735_at',\n",
      "       'X90858_at', 'U57094_at', 'U51990_at', 'D86956_at', 'X79568_at',\n",
      "       'M92287_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the selected top features based on Random Forest feature importance\n",
    "X_selected = X[:, top_n_indices]  # Already selected top 100 features\n",
    "\n",
    "# Split the dataset with selected features into training and test data\n",
    "X_train_selected, X_test_selected, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a new Random Forest model for RFE\n",
    "rf_for_rfe = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Apply RFE to select the most important features out of the top 100 selected features\n",
    "n_features_to_select_rfe = 50  # You can adjust this based on your requirement\n",
    "rfe = RFE(estimator=rf_for_rfe, n_features_to_select=n_features_to_select_rfe)\n",
    "\n",
    "# Fit the RFE model\n",
    "rfe.fit(X_train_selected, Y_train)\n",
    "\n",
    "# Get the selected features after RFE\n",
    "X_train_rfe = rfe.transform(X_train_selected)\n",
    "X_test_rfe = rfe.transform(X_test_selected)\n",
    "\n",
    "# Train a new Random Forest model on the reduced feature set\n",
    "rf_after_rfe = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_after_rfe.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "Y_pred = rf_after_rfe.predict(X_test_rfe)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Print the accuracy of the model after RFE\n",
    "print(\"Accuracy after RFE:\", accuracy)\n",
    "\n",
    "# Print the ranking of features (1 means selected, higher means less important)\n",
    "print(\"Feature ranking after RFE:\")\n",
    "print(rfe.ranking_)\n",
    "\n",
    "# Optionally, print the names of the features that were selected after RFE\n",
    "selected_rfe_feature_names = selected_feature_names[rfe.support_]\n",
    "print(\"Selected features after RFE:\")\n",
    "print(selected_rfe_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Metrices of RF model after RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333\n",
      "Precision: 0.7500\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.8571\n",
      "ROC AUC: 0.9583\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.88      0.96      0.91        15\n",
      "weighted avg       0.95      0.93      0.94        15\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  1]\n",
      " [ 0  3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = rf_after_rfe.predict(X_test_rfe)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate Precision\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Calculate Recall\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate ROC AUC Score\n",
    "roc_auc = roc_auc_score(Y_test, Y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Print Classification Report (Precision, Recall, F1-Score for each class)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "# Print Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after RFE: 1.0\n",
      "Feature ranking after RFE:\n",
      "[2504 6687 6227 ... 6829  185 5817]\n",
      "Confusion Matrix:\n",
      "[[12  0]\n",
      " [ 0  3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "Selected features after RFE:\n",
      "Index(['hum_alu_at', 'AFFX-HUMRGE/M10098_3_at', 'D21261_at',\n",
      "       'HG1612-HT1612_at', 'HG3549-HT3751_at', 'J04164_at', 'L20941_at',\n",
      "       'M11147_at', 'M11722_at', 'M13792_at', 'M19507_at', 'M26602_at',\n",
      "       'M27891_at', 'M31627_at', 'M33680_at', 'M63138_at', 'M69043_at',\n",
      "       'M91036_rna1_at', 'M92287_at', 'M96326_rna1_at', 'U05259_rna1_at',\n",
      "       'U14968_at', 'U46751_at', 'U51004_at', 'X15940_at', 'X17042_at',\n",
      "       'X51466_at', 'X55715_at', 'X59417_at', 'Y00433_at', 'Z23090_at',\n",
      "       'Z26876_at', 'L06797_s_at', 'M25079_s_at', 'HG1428-HT1428_s_at',\n",
      "       'U05255_s_at', 'X03689_s_at', 'M14328_s_at', 'M26708_s_at',\n",
      "       'M28130_rna1_s_at', 'Y00787_s_at', 'Z19554_s_at', 'M27783_s_at',\n",
      "       'X00437_s_at', 'Z48501_s_at', 'X56681_s_at', 'HG2887-HT3031_at',\n",
      "       'U01317_cds4_at', 'X17093_at', 'X69654_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming X and Y are already defined as the features and target labels\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM model with a linear kernel\n",
    "svm_for_rfe = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Apply RFE with the SVM model to select a smaller subset of features\n",
    "n_features_to_select_rfe = 50  # Adjust this based on your requirement\n",
    "rfe = RFE(estimator=svm_for_rfe, n_features_to_select=n_features_to_select_rfe)\n",
    "\n",
    "# Fit the RFE model\n",
    "rfe.fit(X_train, Y_train)\n",
    "\n",
    "# Get the selected features after RFE\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Train a new SVM model on the reduced feature set\n",
    "svm_after_rfe = SVC(kernel='linear', random_state=42)\n",
    "svm_after_rfe.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "Y_pred = svm_after_rfe.predict(X_test_rfe)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Print the accuracy of the model after RFE\n",
    "print(\"Accuracy after RFE:\", accuracy)\n",
    "\n",
    "# Print the ranking of features (1 means selected, higher means less important)\n",
    "print(\"Feature ranking after RFE:\")\n",
    "print(rfe.ranking_)\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "# Correctly print the names of the features that were selected after RFE\n",
    "# Assuming 'df' contains the original dataset with all feature columns and labels\n",
    "# df.columns[:-1] excludes the label column to match the features\n",
    "selected_feature_names_rf = df.columns[:-1]  # Assuming the last column is the label column\n",
    "selected_rfe_feature_names = selected_feature_names_rf[rfe.support_]\n",
    "\n",
    "print(\"Selected features after RFE:\")\n",
    "print(selected_rfe_feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
